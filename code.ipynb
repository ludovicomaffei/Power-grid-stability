{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4icDD3NQN1f"
      },
      "source": [
        "In questa sezione, vengono importate diverse librerie necessarie per eseguire il codice. Queste librerie includono numpy per operazioni\n",
        "matematiche, matplotlib e seaborn per la visualizzazione dei dati, pandas per la manipolazione dei dati, scikit-learn per la\n",
        "scalatura dei dati e la valutazione del modello, keras per la creazione del modello di rete neurale e datetime\n",
        "per calcolare il tempo trascorso durante l'esecuzione del codice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvLWyFvBQL5F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pivY2cTkQTo7"
      },
      "source": [
        "Definizione delle funzioni assessment e correlation_map:\n",
        "Le funzioni assessment e correlation_map sono definite nel codice per la visualizzazione e la generazione di grafici dei dati. assessment genera grafici di distribuzione e dispersione, mentre correlation_map crea una mappa di correlazione come una heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC6olLI6QXgi"
      },
      "outputs": [],
      "source": [
        "def assessment(f_data, f_y_feature, f_x_feature, f_index=-1):\n",
        "\n",
        "    for f_row in f_data:\n",
        "        if f_index >= 0:\n",
        "            f_color = np.where(f_data[f_row].index == f_index,'r','g')\n",
        "            f_hue = None\n",
        "        else:\n",
        "            f_color = 'b'\n",
        "            f_hue = None\n",
        "\n",
        "    f_fig, f_a = plt.subplots(1, 2, figsize=(16,4))\n",
        "\n",
        "    f_chart1 = sns.distplot(f_data[f_x_feature], ax=f_a[0], kde=False, color='g')\n",
        "    f_chart1.set_xlabel(f_x_feature,fontsize=10)\n",
        "\n",
        "    if f_index >= 0:\n",
        "        f_chart2 = plt.scatter(f_data[f_x_feature], f_data[f_y_feature], c=f_color, edgecolors='w')\n",
        "        f_chart2 = plt.xlabel(f_x_feature, fontsize=10)\n",
        "        f_chart2 = plt.ylabel(f_y_feature, fontsize=10)\n",
        "    else:\n",
        "        f_chart2 = sns.scatterplot(x=f_x_feature, y=f_y_feature, data=f_data, hue=f_hue, legend=False)\n",
        "        f_chart2.set_xlabel(f_x_feature,fontsize=10)\n",
        "        f_chart2.set_ylabel(f_y_feature,fontsize=10)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def correlation_map(f_data, f_feature, f_number):\n",
        "    \"\"\"\n",
        "    Develops and displays a heatmap plot referenced to a primary feature of a dataframe, highlighting\n",
        "    the correlation among the 'n' mostly correlated features of the dataframe.\n",
        "\n",
        "    Keyword arguments:\n",
        "\n",
        "    f_data      Tensor containing all relevant features, including the primary.\n",
        "                Pandas dataframe\n",
        "    f_feature   The primary feature.\n",
        "                String\n",
        "    f_number    The number of features most correlated to the primary feature.\n",
        "                Integer\n",
        "    \"\"\"\n",
        "    f_most_correlated = f_data.corr().nlargest(f_number,f_feature)[f_feature].index\n",
        "    f_correlation = f_data[f_most_correlated].corr()\n",
        "\n",
        "    f_mask = np.zeros_like(f_correlation)\n",
        "    f_mask[np.triu_indices_from(f_mask)] = True\n",
        "    with sns.axes_style(\"white\"):\n",
        "        f_fig, f_ax = plt.subplots(figsize=(20, 10))\n",
        "        sns.heatmap(f_correlation, mask=f_mask, vmin=-1, vmax=1, square=True,\n",
        "                    center=0, annot=True, annot_kws={\"size\": 8}, cmap=\"PRGn\")\n",
        "    plt.show()\n",
        "    sns.set()\n",
        "start_time = datetime.now()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDiKpno7QelC"
      },
      "source": [
        "Caricamento dei dati:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO8DDxFkQg1v"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwJqKrfuQkEb"
      },
      "source": [
        "Preparazione dei dati:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZTFCWy0QloE"
      },
      "outputs": [],
      "source": [
        "map1 = {'unstable': 0, 'stable': 1}\n",
        "data['stabf'] = data['stabf'].replace(map1)\n",
        "\n",
        "data = data.sample(frac=1)\n",
        "\n",
        "for column in data.columns:\n",
        "    assessment(data, 'stab', column, -1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWdBOl98Qng_"
      },
      "source": [
        "Calcolo di alcune statistiche. In questa sezione, vengono calcolate alcune statistiche. La funzione skew() calcola l'asimmetria della colonna \"p1\" del dataset. Successivamente, vengono stampate le percentuali di osservazioni \"unstable\" e \"stable\" nel dataset. Infine, viene chiamata la funzione correlation_map per creare una mappa di correlazione basata sulla colonna \"stabf\" del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wz1-vfhWQqrD"
      },
      "outputs": [],
      "source": [
        "data.p1.skew()\n",
        "print(f'Split of \"unstable\" (0) and \"stable\" (1) observations in the original dataset:')\n",
        "print(data['stabf'].value_counts(normalize=True))\n",
        "correlation_map(data, 'stabf', 14)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMvIhZaFQsOR"
      },
      "source": [
        "Preparazione dei dati di addestramento e test.\n",
        "In questa sezione, le caratteristiche (X) e le etichette (y) vengono estratte dal dataset. Successivamente, i dati vengono divisi in set di addestramento e test, con i primi 54000 campioni utilizzati per l'addestramento e il resto per il test. Vengono anche calcolate le percentuali delle etichette nel set di addestramento e test. Infine, i dati vengono convertiti in array numpy e viene eseguita la normalizzazione dei dati utilizzando la classe StandardScaler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZr9BsLQQ0zp"
      },
      "outputs": [],
      "source": [
        "X = data.iloc[:, :12]\n",
        "y = data.iloc[:, 13]\n",
        "\n",
        "X_training = X.iloc[:54000, :]\n",
        "y_training = y.iloc[:54000]\n",
        "\n",
        "X_testing = X.iloc[54000:, :]\n",
        "y_testing = y.iloc[54000:]\n",
        "\n",
        "ratio_training = y_training.value_counts(normalize=True)\n",
        "ratio_testing = y_testing.value_counts(normalize=True)\n",
        "ratio_training, ratio_testing\n",
        "\n",
        "X_training = X_training.values\n",
        "y_training = y_training.values\n",
        "\n",
        "X_testing = X_testing.values\n",
        "y_testing = y_testing.values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_training = scaler.fit_transform(X_training)\n",
        "X_testing = scaler.transform(X_testing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbkc0EB1Q2KC"
      },
      "source": [
        "Creazione e addestramento dell'ANN. In questa sezione, viene creata l'architettura dell'ANN utilizzando la classe Sequential di Keras. Vengono aggiunti quattro strati, tra cui uno strato di input, due strati nascosti e uno strato di output. Viene utilizzata la funzione di attivazione \"relu\" per gli strati nascosti e \"sigmoid\" per lo strato di output. L'ottimizzatore \"adam\" viene utilizzato per minimizzare la funzione di perdita \"binary_crossentropy\". Quindi, il modello viene addestrato utilizzando la tecnica di cross-validation K-Fold con 10 fold. Durante l'addestramento, vengono stampati il valore della perdita e l'accuratezza per ogni round di cross-validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqJYit68RcER"
      },
      "outputs": [],
      "source": [
        "# ANN initialization\n",
        "classifier = Sequential()\n",
        "\n",
        "# Input layer and first hidden layer\n",
        "classifier.add(Dense(units = 24, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\n",
        "\n",
        "# Second hidden layer\n",
        "classifier.add(Dense(units = 24, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "\n",
        "# Third hidden layer\n",
        "classifier.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "\n",
        "# Single-node output layer\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "# ANN compilation\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "cross_val_round = 1\n",
        "print(f'Model evaluation\\n')\n",
        "\n",
        "for train_index, val_index in KFold(10, shuffle=True, random_state=10).split(X_training):\n",
        "    x_train, x_val = X_training[train_index], X_training[val_index]\n",
        "    y_train ,y_val = y_training[train_index], y_training[val_index]\n",
        "    classifier.fit(x_train, y_train, epochs=50, verbose=0)\n",
        "    classifier_loss, classifier_accuracy = classifier.evaluate(x_val, y_val)\n",
        "    print(f'Round {cross_val_round} - Loss: {classifier_loss:.4f} | Accuracy: {classifier_accuracy * 100:.2f} %')\n",
        "    cross_val_round += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g56-eA3fRjJq"
      },
      "source": [
        "In questa sezione, viene utilizzato il modello addestrato per effettuare previsioni sui dati di test. Successivamente, viene creata una matrice di confusione per valutare le prestazioni del modello. Viene calcolata l'accuratezza basata sulla matrice di confusione. Infine, vengono stampati il tempo di inizio, il tempo di fine e il tempo totale trascorso durante l'esecuzione del codice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMEf9EkLQ_vX"
      },
      "outputs": [],
      "source": [
        "y_pred = classifier.predict(X_testing)\n",
        "y_pred[y_pred <= 0.5] = 0\n",
        "y_pred[y_pred > 0.5] = 1\n",
        "\n",
        "cm = pd.DataFrame(data=confusion_matrix(y_testing, y_pred, labels=[0, 1]),\n",
        "                  index=[\"Actual Unstable\", \"Actual Stable\"],\n",
        "                  columns=[\"Predicted Unstable\", \"Predicted Stable\"])\n",
        "cm\n",
        "\n",
        "print(f'Accuracy per the confusion matrix: {((cm.iloc[0, 0] + cm.iloc[1, 1]) / len(y_testing) * 100):.2f}%')\n",
        "\n",
        "end_time = datetime.now()\n",
        "\n",
        "print('\\nStart time', start_time)\n",
        "print('End time', end_time)\n",
        "print('Time elapsed', end_time - start_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jap0UQgnRBR4"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
